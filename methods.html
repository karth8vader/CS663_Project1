<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Methods | Sign Language Tutorial</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!-- Bootstrap -->
        <link
            href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
            rel="stylesheet"
        />
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
            }
            .navbar {
                margin-bottom: 20px;
            }
            .content {
                max-width: 1000px;
                margin: auto;
                padding: 20px;
            }
            .image-block {
                margin: 20px 0;
                text-align: center;
            }
            .image-block img {
                max-width: 100%;
                border-radius: 8px;
            }
            .badge-pill {
                border-radius: 50px;
            }
            pre code {
                background: #f7f7f7;
                display: block;
                padding: 14px;
                border-radius: 8px;
                overflow-x: auto;
            }
            .callout {
                background: #eef7ff;
                border-left: 6px solid #0d6efd;
                padding: 16px;
                margin: 20px 0;
            }
            .tradeoffs td,
            .tradeoffs th {
                vertical-align: top;
            }
            .small-muted {
                font-size: 0.95rem;
                color: #666;
            }
            .copy-btn {
                position: absolute;
                right: 16px;
                top: 12px;
            }
            .code-wrap {
                position: relative;
            }
        </style>
    </head>
    <body>
        <!-- Nav -->
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
            <div class="container-fluid">
                <a class="navbar-brand" href="index.html"
                    >Sign Language Tutorial</a
                >
                <button
                    class="navbar-toggler"
                    type="button"
                    data-bs-toggle="collapse"
                    data-bs-target="#nav"
                    aria-controls="nav"
                    aria-expanded="false"
                    aria-label="Toggle navigation"
                >
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="nav">
                    <ul class="navbar-nav me-auto">
                        <li class="nav-item">
                            <a class="nav-link" href="intro.html"
                                >Introduction</a
                            >
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="background.html"
                                >Background</a
                            >
                        </li>
                        <li class="nav-item">
                            <a class="nav-link active" href="methods.html"
                                >Methods</a
                            >
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="applications.html"
                                >Applications</a
                            >
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="demo.html">Demo</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="references.html"
                                >References</a
                            >
                        </li>
                    </ul>
                </div>
            </div>
        </nav>

        <!-- Content -->
        <div class="content">
            <h1 class="text-center">
                Methods: Computer Vision for Teaching Sign Language
            </h1>
            <p class="text-center small-muted">
                Pipeline overview, key algorithms, code, datasets, evaluation,
                and limitations.
            </p>

            <!-- Pipeline Overview -->
            <section class="mt-4">
                <h2>1) End-to-End Pipeline</h2>
                <div class="row">
                    <div class="col-lg-7">
                        <ol>
                            <li>
                                <strong>Capture:</strong> Webcam streams RGB
                                frames (typ. 640×480 @ 30 FPS).
                            </li>
                            <li>
                                <strong
                                    >Hand Detection &amp; Pose
                                    Estimation:</strong
                                >
                                Locate hands and extract 2D landmarks (e.g., 21
                                points per hand).
                            </li>
                            <li>
                                <strong>Feature Engineering:</strong> Normalize
                                landmarks (scale/rotate), derive
                                angles/distances, or use raw coords.
                            </li>
                            <li>
                                <strong>Recognition:</strong>
                                <ul>
                                    <li>
                                        <em>Static signs:</em> feed a single
                                        frame’s features to a classifier
                                        (CNN/MLP/SVM).
                                    </li>
                                    <li>
                                        <em>Dynamic signs:</em> feed landmark
                                        sequences to a temporal model
                                        (LSTM/GRU/Transformer/TCN).
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <strong>Feedback:</strong> On-screen cues,
                                correctness score, and playful hints for kids
                                (emojis/sounds).
                            </li>
                            <li>
                                <strong>Logging &amp; Adaptation:</strong> Track
                                progress; adapt difficulty (personalized
                                pacing).
                            </li>
                        </ol>
                    </div>
                    <div class="col-lg-5">
                        <div class="image-block">
                            <img
                                src="images/cv_pipeline_diagram.png"
                                alt="CV pipeline"
                            />
                            <p class="text-muted">
                                Figure: Camera → Pose → Features → Classifier →
                                Child-friendly feedback.
                            </p>
                        </div>
                    </div>
                </div>
                <div class="callout">
                    <strong>Design for children:</strong> minimize latency
                    (&lt;100ms), keep UI elements large, provide positive
                    reinforcement, and avoid complex text.
                </div>
            </section>

            <!-- Algorithms -->
            <section class="mt-4">
                <h2>2) Core Algorithms</h2>

                <h5 class="mt-3">
                    <span class="badge bg-primary badge-pill">A</span> Hand Pose
                    Estimation
                </h5>
                <p>
                    Use a real-time hand detector + landmark regressor (e.g.,
                    MediaPipe Hands-like pipeline) to obtain 21 keypoints per
                    hand. Normalize (translate to wrist, scale by palm size,
                    rotate to a canonical frame) to reduce variation in scale
                    and camera angle.
                </p>

                <h5 class="mt-3">
                    <span class="badge bg-primary badge-pill">B</span> Static
                    Sign Classification
                </h5>
                <p>
                    For alphabetic/static signs (A, B, C, …), train a classifier
                    on single-frame features. Options:
                </p>
                <ul>
                    <li>
                        <strong>MLP/CNN:</strong> Works well on normalized 2D
                        keypoints or cropped hand images.
                    </li>
                    <li>
                        <strong>Classical baselines:</strong> SVM, Random Forest
                        on engineered angles/distances for quick baselines.
                    </li>
                </ul>

                <h5 class="mt-3">
                    <span class="badge bg-primary badge-pill">C</span> Dynamic
                    Sign Recognition
                </h5>
                <p>For motion-based signs (e.g., “WHERE”), model sequences:</p>
                <ul>
                    <li>
                        <strong>LSTM/GRU:</strong> straightforward for temporal
                        dependencies.
                    </li>
                    <li>
                        <strong>Temporal CNN (TCN):</strong> efficient, good
                        receptive fields.
                    </li>
                    <li>
                        <strong>Transformer:</strong> strong for longer
                        gestures; needs more data/regularization.
                    </li>
                    <li>
                        <strong>Classical:</strong> HMM/DTW as interpretable
                        baselines on landmark sequences.
                    </li>
                </ul>
            </section>

            <!-- Trade-offs -->
            <section class="mt-4">
                <h2>3) Alternatives &amp; Trade-offs</h2>
                <div class="table-responsive">
                    <table class="table table-bordered tradeoffs">
                        <thead class="table-light">
                            <tr>
                                <th>Approach</th>
                                <th>Pros</th>
                                <th>Cons</th>
                                <th>Use When</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Keypoints → MLP/CNN</td>
                                <td>
                                    Fast, compact, robust to background; smaller
                                    models
                                </td>
                                <td>
                                    Depends on landmark quality; may miss finger
                                    texture cues
                                </td>
                                <td>Low-latency, browser/CPU scenarios</td>
                            </tr>
                            <tr>
                                <td>Image crops → CNN</td>
                                <td>
                                    Uses rich pixel cues; good for subtle shapes
                                </td>
                                <td>
                                    Heavier inference; sensitive to
                                    lighting/background
                                </td>
                                <td>GPU available; complex hand shapes</td>
                            </tr>
                            <tr>
                                <td>Sequences → LSTM/GRU</td>
                                <td>Simple, effective temporal modeling</td>
                                <td>
                                    May struggle with long-range dependencies
                                </td>
                                <td>Short dynamic signs (≤2–3s)</td>
                            </tr>
                            <tr>
                                <td>Sequences → Transformer</td>
                                <td>
                                    Great for longer gestures, global context
                                </td>
                                <td>Data-hungry; higher latency</td>
                                <td>
                                    Richer phrases; adequate data &amp; hardware
                                </td>
                            </tr>
                            <tr>
                                <td>HMM/DTW baselines</td>
                                <td>Interpretable; small data friendly</td>
                                <td>
                                    Lower ceiling accuracy; feature crafting
                                </td>
                                <td>Educational comparison; low-resource</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>

            <!-- Code Example -->
            <section class="mt-4">
                <h2>
                    4) Code: Real-Time Hand Tracking &amp; Static Sign
                    Classification
                </h2>
                <p class="small-muted">
                    Below is a minimal Python example using
                    <code>mediapipe</code> and <code>opencv-python</code>. It
                    draws landmarks and (optionally) calls a tiny MLP to
                    classify a static sign from the 21×(x,y,z) keypoints.
                    Replace the dummy classifier with your trained model.
                </p>

                <div class="code-wrap">
                    <button
                        class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCode()"
                    >
                        Copy
                    </button>
                    <pre><code id="code-block" class="language-python"># pip install mediapipe opencv-python numpy joblib
import cv2, numpy as np, joblib
import mediapipe as mp

# --- Load pose estimator ---
mp_hands = mp.solutions.hands
mp_draw  = mp.solutions.drawing_utils
hands = mp_hands.Hands(static_image_mode=False,
                       max_num_hands=1,
                       min_detection_confidence=0.6,
                       min_tracking_confidence=0.6)

# --- (Optional) Load a pre-trained classifier on 63-dim features (21 keypoints * x,y,z) ---
# clf = joblib.load("models/static_sign_mlp.joblib")
clf = None  # Replace with your model to enable predictions

def landmarks_to_features(lm, img_w, img_h):
    # Convert normalized [0,1] coords -> pixel, then normalize: translate to wrist, scale by palm size
    pts = []
    for i in range(21):
        x = lm.landmark[i].x * img_w
        y = lm.landmark[i].y * img_h
        z = lm.landmark[i].z * max(img_w, img_h)
        pts.append([x, y, z])
    pts = np.array(pts, dtype=np.float32)

    wrist = pts[0]
    pts -= wrist  # translate
    palm_size = np.linalg.norm(pts[9] - pts[0]) + 1e-6  # wrist to middle MCP as scale
    pts /= palm_size

    # Optional: rotate to canonical orientation using PCA on (x,y)
    P = pts[:, :2]
    P -= P.mean(axis=0)
    U, S, Vt = np.linalg.svd(P, full_matrices=False)
    R = Vt  # 2x2
    P_rot = P @ R.T
    feats = np.concatenate([P_rot, pts[:, 2:3]], axis=1).reshape(-1)  # 21 * (x', y', z)
    return feats

cap = cv2.VideoCapture(0)
label, prob = "", 0.0

while True:
    ok, frame = cap.read()
    if not ok: break
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    res = hands.process(rgb)

    if res.multi_hand_landmarks:
        for lm in res.multi_hand_landmarks:
            mp_draw.draw_landmarks(frame, lm, mp_hands.HAND_CONNECTIONS)

            if clf is not None:
                h, w = frame.shape[:2]
                feats = landmarks_to_features(lm, w, h)
                # Example: clf.predict_proba expects shape (1, 63)
                proba = clf.predict_proba([feats])[0]
                idx = int(np.argmax(proba))
                label = f"Class {idx}"
                prob = float(proba[idx])

    if label:
        cv2.rectangle(frame, (10,10), (260,60), (0,0,0), -1)
        txt = f"{label}  {prob:.2f}"
        cv2.putText(frame, txt, (20,45), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)

    cv2.imshow("Hand Tracking + Static Sign Classification (Press q to quit)", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'): break

cap.release()
cv2.destroyAllWindows()</code></pre>
                </div>
                <p class="small-muted">
                    <strong>How it works:</strong> detects a hand, extracts 21
                    landmarks, normalizes them (translation/scale/rotation), and
                    feeds them to a classifier. For training, collect labeled
                    frames of each static sign, compute features as above, then
                    fit an MLP/SVM. For dynamic signs, aggregate features across
                    time and train a temporal model (see Section 2C).
                </p>
            </section>

            <!-- Hardware -->
            <section class="mt-4">
                <h2>5) Hardware &amp; Runtime Considerations</h2>
                <ul>
                    <li>
                        <strong>Webcam:</strong> Standard 720p works; ensure
                        stable lighting and plain background for kids.
                    </li>
                    <li>
                        <strong>CPU vs GPU:</strong> Keypoint-based models
                        typically run fine on CPU; image-based CNNs benefit from
                        GPU.
                    </li>
                    <li>
                        <strong>Mobile/Browser:</strong> Consider WebGL/WebGPU
                        or TF.js if deploying in-browser; prioritize
                        low-latency.
                    </li>
                    <li>
                        <strong>Edge cases:</strong> small hands, fast motion,
                        occlusions (two hands overlapping), left/right
                        mirroring.
                    </li>
                </ul>
            </section>

            <!-- Data -->
            <section class="mt-4">
                <h2>6) Datasets &amp; Labeling</h2>
                <p>
                    For child-centric tools, you may need to curate a small,
                    focused dataset:
                </p>
                <ul>
                    <li>
                        <strong>Static signs:</strong> capture 100–300 frames
                        per class across varied lighting and hand sizes.
                    </li>
                    <li>
                        <strong>Dynamic signs:</strong> record short clips
                        (1–3s); store landmark sequences to reduce storage.
                    </li>
                    <li>
                        <strong>Augmentation:</strong> brightness/contrast
                        jitter, slight rotation/scale, temporal jitter for
                        sequences.
                    </li>
                    <li>
                        <strong>Ethics/Consent:</strong> obtain consent for any
                        child data; anonymize faces; follow IRB/school policies
                        where applicable.
                    </li>
                </ul>
            </section>

            <!-- Evaluation -->
            <section class="mt-4">
                <h2>7) Evaluation Metrics</h2>
                <ul>
                    <li>
                        <strong>Accuracy / F1-score:</strong> per-class and
                        macro averages.
                    </li>
                    <li>
                        <strong>Latency:</strong> end-to-end ms/frame; target
                        &lt;100ms for smooth feedback.
                    </li>
                    <li>
                        <strong>Robustness:</strong> test across lighting,
                        backgrounds, skin tones, hand sizes (child variance).
                    </li>
                    <li>
                        <strong>UX metrics:</strong> time-to-correct-gesture,
                        child engagement (task completion, smile/affect if
                        appropriate), and caregiver/teacher feedback.
                    </li>
                </ul>
            </section>

            <!-- Limitations -->
            <section class="mt-4">
                <h2>8) Limitations &amp; Mitigations</h2>
                <ul>
                    <li>
                        <strong>Occlusions:</strong> ask learner to keep hands
                        apart; show on-screen hand position guides.
                    </li>
                    <li>
                        <strong>Lighting sensitivity:</strong> auto-exposure can
                        jitter; add exposure lock or guidance.
                    </li>
                    <li>
                        <strong>Generalization:</strong> models trained on
                        adults may underperform on children’s hands—collect
                        child data or augment sizes.
                    </li>
                    <li>
                        <strong>Dynamic signs complexity:</strong> require
                        temporal models and more data; scaffold curriculum
                        (start with static alphabet).
                    </li>
                </ul>
            </section>

            <!-- Image slot -->
            <div class="image-block">
                <img
                    src="images/cnn_architecture.png"
                    alt="CNN diagram for static sign classification"
                />
                <p class="text-muted">
                    Figure: Example CNN/MLP head for classifying normalized
                    keypoints or cropped hand images.
                </p>
            </div>
        </div>

        <!-- Footer -->
        <footer class="text-center mt-5 mb-3">
            <p>
                &copy; 2025 Teaching Children Sign Language Tutorial | Built
                with HTML, CSS, and Bootstrap
            </p>
        </footer>

        <!-- JS -->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
        <script>
            function copyCode() {
                const el = document.getElementById("code-block");
                const range = document.createRange();
                range.selectNode(el);
                const sel = window.getSelection();
                sel.removeAllRanges();
                sel.addRange(range);
                try {
                    document.execCommand("copy");
                } catch (e) {}
                sel.removeAllRanges();
                alert("Code copied to clipboard!");
            }
        </script>
    </body>
</html>
